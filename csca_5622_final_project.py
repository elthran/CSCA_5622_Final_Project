# -*- coding: utf-8 -*-
"""CSCA_5622_final_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XiY75iDfS6O7GhVJNu8MLyPyVcyaI9WD

# Introduction

This notebook will be examining the "House Prices - Advanced Regression Techniques" data from this Kaggle competition: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data

It has training data describing a lot of house sales, and the amount of money that the house was sold for. There is also some extra test data which is similar but does not have the sale price.

I am going to combine this data, clean and visualize it, then use the training data to build some models to try to predict the sale price as accurately as I can. Then I will submit the test data with mypredictions to Kaggle to see if my results are valid.

The data should be attached in a folder called 'data/' which includes a "data_description.txt" file from Kaggle which explains all of the features.

## Step 1: Pull the data
"""

from matplotlib import pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score


train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')

# Add a dummy column to test for our target so we can combine the data
test["SalePrice"] = -1
df = pd.concat([train, test], axis=0).reset_index(drop=True)

# Get the shape of the data to see how many rows and columns we have
# Since this is Kaggle competition data, we get an extra test dataset that is missing the target feature
# Even though it's not needed for this project, we will try our best and submit in the competition
# Since kaggle will give us a score and help sanity check our results
print("Train shape:", train.shape)
print("Test shape:", test.shape)
print("Combined shape:", df.shape)

"""# EDA"""

## Let's get a simple look at our data
df.describe().T.round(3)

"""## Step 1: Deal with Null Data"""

# We can see that there are some null values in our data. This will cause issues for some models.
# Let's get a count of what is null
null_counts = df.isnull().sum().sort_values(ascending=False)
null_counts = null_counts[null_counts > 0]
null_counts

# I'm going to remove features that have a large percentage null, since they are very unreliable
null_columns_to_drop = null_counts[null_counts > 400].index
df = df.drop(columns=null_columns_to_drop, axis=1)

# Let's get the new list
null_counts = df.isnull().sum().sort_values(ascending=False)
null_counts = null_counts[null_counts > 0]
null_counts

# Now our remaining features have small null counts.
# I'm going to replace them with the average values since I don't have enough
# domain knowledge to try and do better
for column in df.columns:
    # Check if the column type is numeric (integer or float)
    if pd.api.types.is_numeric_dtype(df[column]):
        # Fill missing values with the median of the column
        df[column] = df[column].fillna(value=df[column].median())
    # Check if the column type is an object (string)
    else:
        # Fill missing values with the most common value
        df[column] = df[column].mode()[0]

# Let's make sure we have no nulls now
null_counts = df.isnull().sum().sort_values(ascending=False)
null_counts = null_counts[null_counts > 0]
null_counts

# Let's next look if the data is all the correct type and format before we visualize it
df.info()

# I think those all look correct. Looking at the data descriptions ('data/data_description.txt') there is some cleaning we can do

# Let's get any column with 3 of fewer unique values and see if we can clean them up
# First we can drop any column that only ever has a single unique value since that is useless
single_unique_columns_to_drop = df.columns[df.nunique() == 1]
df = df.drop(columns=single_unique_columns_to_drop)

columns_with_few_unique_values = df.columns[df.nunique() <= 3]
for column in columns_with_few_unique_values:
  print(df[column].value_counts())

# These look acceptable since they are integer columns

"""## Step 2: Visualize and Clean Any Remaining Features"""

# Now that we have no nulls, let's look at our features. Let's start with the target
# We will plot a simple histogram to see the distrubition


sns.histplot(data=train, x='SalePrice', bins=100, kde=True)

# We can see that there are extreme outliers on the right side and a long tail

# Now let's histograms of all the other features to see if anything stands out
train.hist(bins=100, figsize=(20, 20))

# Some of these have ridiculous skews and will be terrible for our small dataset. Let's clean them up

# Let's convert PoolArea into a boolean HasPool
df["HasPool"] = df["PoolArea"] > 0
df = df.drop("PoolArea", axis=1)

# LowQualFinSF, BsmtFinSF2, MiscVal have 99% 0 so these won't be useful
df = df.drop(["LowQualFinSF", "BsmtFinSF2", "MiscVal"], axis=1)

# Now let's see how these features all correlate to our target

# First remove the test data since it has our -1 target placeholder
df_without_test = df[df["SalePrice"] > 0]

# Create the plot
plt.figure(figsize=(12, 10))

# Build the correlation matrix
correlation_matrix = df_without_test.corr()['SalePrice'].sort_values(ascending=False).to_frame()

# Plot the heatmap
sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, vmin=-1, vmax=1,
            linewidths=0.5, linecolor='black')

# Set the title of the heatmap
plt.title('Correlation of Features to Target')

# Now we can start to understand the data and how it relates

"""# Model Training

## Step 1: Split the data
"""

# Unsplit our data that we started with
train_df = df[df["SalePrice"] > 0]
test_df = df[df["SalePrice"] == -1]
# Drop the target from our test data since it was a placeholder
test_df = test_df.drop("SalePrice", axis=1)
del df

# This should have the same row counts as we started with (1460 and 1459) since we shouldn't drop rows
print("Tran data:", train_df.shape)
print("Test data:", test_df.shape)

# We can't use the test_df for anything since it doesn't have the target labels
# X is our feature columns
X = train_df.drop("SalePrice", axis=1)
# y is our target column
y = train_df["SalePrice"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

print("Our X data should have the same features as our test_df")
print("X_train:", X_train.shape)
print("X_test:", X_test.shape)

print("Our y data should have a single target")
print("y_train:", y_train.shape)
print("y_test:", y_test.shape)

# Let's create a dictionary for all of our results so we can compare different models
model_results = {}

"""## Train Baseline (Decision Tree) Model"""

from sklearn.tree import DecisionTreeRegressor


# Initialize the DecisionTreeRegressor (since its real data) model
decision_tree_model = DecisionTreeRegressor(random_state=42)

# Train the model on the training data
decision_tree_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = decision_tree_model.predict(X_test)

# Evaluate the model (and save to our results)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
results = {"mse": mse, "r2": r2}
model_results["decision_tree_model"] = results

# Print evaluation metrics
print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

"""## Train Random Forest Model"""

from sklearn.ensemble import RandomForestRegressor


# Initialize the RandomForestRegressor (since its real data) model
random_forest_model = RandomForestRegressor(random_state=42)

# Train the model on the training data
random_forest_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = random_forest_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
results = {"mse": mse, "r2": r2}
model_results["random_forest_model"] = results

# Print evaluation metrics
print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

"""## Train Linear Regression Model"""

from sklearn.linear_model import LinearRegression


# Initialize the LinearRegression model
linear_regression_model = LinearRegression()

# Train the model on the training data
linear_regression_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = linear_regression_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
results = {"mse": mse, "r2": r2}
model_results["linear_regression_model"] = results

# Print evaluation metrics
print(f"Mean Squared Error (Linear Regression): {mse}")
print(f"R-squared (Linear Regression): {r2}")

"""## Train SVM (Support Vector Regression) Model"""

from sklearn.svm import SVR


# Initialize the SVR model
svm_model = SVR(kernel='linear')

# Train the model on the training data
svm_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = svm_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
results = {"mse": mse, "r2": r2}
model_results["svm_model"] = results

# Print evaluation metrics
print(f"Mean Squared Error (SVM): {mse}")
print(f"R-squared (SVM): {r2}")

"""## Train KNN Model"""

from sklearn.neighbors import KNeighborsRegressor


# Since knn has hyper-parameters, I will attempt a few simple ones
for n_neighbours in range(3, 6):
  # Initialize the KNeighborsRegressor model
  knn_model = KNeighborsRegressor(n_neighbors=n_neighbours)

  # Train the model on the training data
  knn_model.fit(X_train, y_train)

  # Make predictions on the test data
  y_pred = knn_model.predict(X_test)

  # Evaluate the model
  mse = mean_squared_error(y_test, y_pred)
  r2 = r2_score(y_test, y_pred)
  results = {"mse": mse, "r2": r2}
  model_results[f"knn_model_{n_neighbours}_neighbours"] = results

  # Print evaluation metrics
  print(f"Mean Squared Error (k-NN_{n_neighbours}): {mse}")
  print(f"R-squared (k-NN_{n_neighbours}): {r2}")

"""## Model Refinement and Hyper Parameter Tuning"""

# First I want to compare all of the simple models to the baseline model and see which model type is effective on this data
# And then I will start tuning the better models to maximize their accuracy

# Extract model names, MSE and R² values from my dictionary of results
models = list(model_results.keys())
mse_values = []
r2_values = []
for model in models:
  results = model_results[model]
  mse_values.append(results["mse"])
  r2_values.append(results["r2"])

# Create a nice large plot for easy visualization
fig, ax1 = plt.subplots(figsize=(16, 8))

# Bar plot for each MSE
ax1.set_title('Model Performance Comparison', fontsize=16)
ax1.set_xlabel('Models', fontsize=14)
ax1.set_ylabel('Mean Squared Error (MSE)', fontsize=14, color='tab:blue')
ax1.bar(models, mse_values, color='tab:blue', alpha=0.6, label='MSE')
ax1.tick_params(axis='y', labelcolor='tab:blue')

# Creating a second y-axis to plot R-squared
ax2 = ax1.twinx()
ax2.set_ylabel('R-squared (R²)', fontsize=14, color='tab:green')
ax2.plot(models, r2_values, color='tab:green', marker='o', linestyle='-', linewidth=2, label='R²')
ax2.tick_params(axis='y', labelcolor='tab:green')

# Adding legends
fig.tight_layout()
fig.legend(loc='upper left', bbox_to_anchor=(0.1,0.9), bbox_transform=ax1.transAxes)

"""From the graph it's clear that Random Forest is a very strong model, having both the lowest mean squared error as well as the highest r_squared score.
KNN models performed about similar to our baseline model. Linear Regression and SVM were in the middle and might do well with tuning. But we will focus on RAndom Forest for our final model for this project.
"""

from sklearn.model_selection import GridSearchCV


# Let's do a Grid Search on our RAndom Forest model to try and get the best parameters

# Define the parameter grid to search over
# I tested a much larger grid search locally and
# am testing a smaller one in this notebook for reference
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['auto', 'sqrt'],
    'bootstrap': [False],
}

# Initialize the GridSearchCV with the model and the parameter grid
grid_search = GridSearchCV(estimator=random_forest_model,
                           param_grid=param_grid,
                           scoring='neg_mean_squared_error',
                           cv=5, n_jobs=-1, verbose=2)

# Perform the grid search on the training data
grid_search.fit(X_train, y_train)

# Get the best parameters and the best model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Make predictions on the test data using the best model
y_pred = best_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
results = {"mse": mse, "r2": r2}
model_results[f"best_estimator"] = results

# Print the best parameters and evaluation metrics
print(f"Best Parameters: {best_params}")
print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Let's re-examine the results and add our best estimator from the grid search for reference

# Extract model names, MSE and R² values from my dictionary of results
models = list(model_results.keys())
mse_values = []
r2_values = []
for model in models:
  results = model_results[model]
  mse_values.append(results["mse"])
  r2_values.append(results["r2"])

# Create a nice large plot for easy visualization
fig, ax1 = plt.subplots(figsize=(18, 8))

# Bar plot for each MSE
ax1.set_title('Model Performance Comparison', fontsize=16)
ax1.set_xlabel('Models', fontsize=14)
ax1.set_ylabel('Mean Squared Error (MSE)', fontsize=14, color='tab:blue')
ax1.bar(models, mse_values, color='tab:blue', alpha=0.6, label='MSE')
ax1.tick_params(axis='y', labelcolor='tab:blue')

# Creating a second y-axis to plot R-squared
ax2 = ax1.twinx()
ax2.set_ylabel('R-squared (R²)', fontsize=14, color='tab:green')
ax2.plot(models, r2_values, color='tab:green', marker='o', linestyle='-', linewidth=2, label='R²')
ax2.tick_params(axis='y', labelcolor='tab:green')

# Adding legends
fig.tight_layout()
fig.legend(loc='upper left', bbox_to_anchor=(0.1,0.9), bbox_transform=ax1.transAxes)

"""## Predict on the Test Data"""

# Since this is a kaggle competition dataset, let's use our model to predict the actual results

# Generate predictions on test data
test_predict = best_model.predict(test_df)

# Generate an output csv file to submit to Kaggle
prediction_csv = pd.DataFrame({
        "Id": test_df["Id"],
        "SalePrice": test_predict
    })
prediction_csv.to_csv('submission.csv', index=False)
print(prediction_csv)

# It was passable but not amazing lol. I got #3190 on the leaderboard

